# Convert custom dataset SAM3_iOCT generated by build_sam3_image_train to COCO format for SAM3 training
import os
import json
import numpy as np
from PIL import Image
from pycocotools import mask as mask_util

def convert_split(split):
    img_dir = f"./datasets/SAM3_iOCT/{split}/JPEGImages"
    ann_dir = f"./datasets/SAM3_iOCT/{split}/Annotations"
    output_json = f"./datasets/SAM3_iOCT/{split}/{split}.json"

    categories = [
        {"id": 1, "name": "tissue"},
        {"id": 2, "name": "tool"},
        {"id": 3, "name": "artifact"}
    ]

    images = []
    annotations = []

    ann_id = 1
    img_id = 1

    for fname in sorted(os.listdir(img_dir)):
        if not fname.endswith(".png"):
            continue

        image_path = os.path.join(img_dir, fname)
        ann_path = os.path.join(ann_dir, fname)

        img = Image.open(image_path)
        w, h = img.size

        mask = np.array(Image.open(ann_path))  # uint8 0/1/2/3

        images.append({
            "id": img_id,
            "file_name": fname,
            "width": w,
            "height": h,
            "is_instance_exhaustive": True,   # ‚≠ê REQUIRED by SAM3 evaluator
            "is_pixel_exhaustive": True       # (optional but recommended)
        })

        # For each class, create one annotation (semantic mask)
        for cls_id in [1, 2, 3]:
            bin_mask = (mask == cls_id).astype(np.uint8)

            if bin_mask.sum() == 0:
                continue  # class not present

            # RLE encoding
            rle = mask_util.encode(np.asfortranarray(bin_mask))
            rle["counts"] = rle["counts"].decode("ascii")

            # Bbox (xywh)
            ys, xs = np.where(bin_mask == 1)
            xmin, xmax = xs.min(), xs.max()
            ymin, ymax = ys.min(), ys.max()
            bbox = [
                float(xmin),
                float(ymin),
                float(xmax - xmin + 1),
                float(ymax - ymin + 1)
            ]
            area = float(bin_mask.sum())

            annotations.append({
                "id": ann_id,
                "image_id": img_id,
                "category_id": cls_id,
                "segmentation": rle,
                "area": area,
                "bbox": bbox,
                "iscrowd": 0
            })

            ann_id += 1

        img_id += 1

    dataset = {
        "images": images,
        "annotations": annotations,
        "categories": categories
    }

    with open(output_json, "w") as f:
        json.dump(dataset, f, indent=2)

    print(f"Saved {output_json}, #images={len(images)}, #annotations={len(annotations)}")


# Convert both train and val
convert_split("train")
convert_split("val")
